{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import random\n",
    "import shap\n",
    "import torch\n",
    "import math\n",
    "import torch.nn as nn \n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from torch import optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.metrics as metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import re\n",
    "\n",
    "\n",
    "\n",
    "from transformers import BertTokenizer, BertModel, XLMTokenizer, XLMConfig, XLMModel, \\\n",
    "    AlbertTokenizer, AlbertModel, RobertaTokenizer, RobertaConfig, RobertaModel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def atoi(text):\n",
    "    return int(text) if text.isdigit() else text\n",
    "def natural_keys(text):\n",
    "    return [atoi(c) for c in re.split('(\\d+)',text) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "LLM = \"bert-base-uncased\"\n",
    "# LLM = \"bert-large-uncased\"\n",
    "# LLM = \"albert\"\n",
    "# LLM = \"roberta\"\n",
    "# LLM = \"xlm\"\n",
    "\n",
    "def change_llm(LLM):\n",
    "\n",
    "    if \"bert-\" in LLM:\n",
    "        tokenizer = BertTokenizer.from_pretrained(LLM)\n",
    "        llm = BertModel.from_pretrained(LLM)\n",
    "\n",
    "\n",
    "    if LLM == \"albert\":\n",
    "        tokenizer = AlbertTokenizer.from_pretrained('albert-base-v2')\n",
    "        llm = AlbertModel.from_pretrained('albert-base-v2')\n",
    "\n",
    "    if LLM == \"roberta\":\n",
    "        tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "        configuration = RobertaConfig()\n",
    "        llm = RobertaModel(configuration)\n",
    "\n",
    "    if LLM == \"xlm\":\n",
    "        tokenizer = XLMTokenizer.from_pretrained(\"xlm-mlm-en-2048\")\n",
    "        configuration = XLMConfig()\n",
    "        llm = XLMModel(configuration)\n",
    "\n",
    "\n",
    "    llm.eval()\n",
    "    sentence = \"Paris is a beautiful city\" \n",
    "    tok = tokenizer(sentence)\n",
    "    encoded_layers = llm(torch.Tensor([tok.input_ids]).to(torch.int), attention_mask=torch.Tensor([tok.attention_mask]))\n",
    "    llm_size = (encoded_layers.last_hidden_state)[:, 0, :].size()[1]\n",
    "\n",
    "    return tokenizer, llm, llm_size\n",
    "tokenizer, llm, llm_size = change_llm(LLM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lv1_lv3 = dict({0:0, 1:0, 2:0, 3:0, 4:0, 5:1, 6:1, 7:1, 8:1, 9:1, 10:1, 11:1, 12:1, 13:2, 14:2, 15:2, 16:2, 17:2, 18:2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DATA():\n",
    "\n",
    "    def __init__(self):\n",
    "        self.dataset_llm = []\n",
    "        self.dataset_opensmile = []\n",
    "        self.dataset_cps_f = []\n",
    "        self.dataset_cps_i = []\n",
    "        self.dataset_action = []\n",
    "        self.dataset_gamr = []\n",
    "\n",
    "        self.targets = []\n",
    "\n",
    "    def openLLM(self, filename):\n",
    "        data = pd.read_csv(filename)\n",
    "        for i in range(len(data)):\n",
    "            self.dataset_llm.append(np.asarray(data.iloc[i]).tolist())\n",
    "    \n",
    "    \n",
    "    def openSmile(self,filename):\n",
    "        data = pd.read_csv(filename)\n",
    "        for i in range (data.shape[0]):\n",
    "            row=data[data['file']==f'C:\\\\Users\\\\bradf\\\\OneDrive - Colostate\\\\Research\\\\Initial Observations for Fib Weights\\\\Data\\\\Segment Analysis\\\\{filename[filename.index(\"Group_\"):filename.index(\"Group_\")+8]}\\\\segments_oracle\\\\{filename[filename.index(\"Group_\"):filename.index(\"Group_\")+8]}_'+str(i)+'.wav']\n",
    "            tensor=np.asarray(row.values[0][3:91],dtype=np.float32).tolist()\n",
    "            self.dataset_opensmile.append(tensor)\n",
    "\n",
    "\n",
    "    def openCPS(self, filename):\n",
    "        data = pd.read_csv(filename).fillna(0)\n",
    "        for row in range(data.shape[0]):\n",
    "            cps = data.iloc[row, 8:].fillna(0)\n",
    "            self.dataset_cps_i.append(list(cps.values))\n",
    "            cps_f = [0, 0, 0]\n",
    "            for i, c in enumerate(list(cps.values)):\n",
    "                if c == 1:\n",
    "                    cps_f[lv1_lv3[i]] = 1\n",
    "            self.dataset_cps_f.append(cps_f)\n",
    "\n",
    "    def openAction(self, filename):\n",
    "        data = pd.read_csv(filename, header=None).fillna(0)\n",
    "        for row in range(data.shape[0]):\n",
    "            self.dataset_action.append(data.iloc[row].to_list())\n",
    "\n",
    "\n",
    "\n",
    "    def openGAMR(self, filename):\n",
    "        data = pd.read_csv(filename, header=None).fillna(0)\n",
    "        for row in range(data.shape[0]):\n",
    "            self.dataset_gamr.append(data.iloc[row].to_list())\n",
    "        \n",
    "\n",
    "    def openTarget(self,filename):\n",
    "        data = pd.read_csv(filename)\n",
    "        for row in range(data.shape[0]):\n",
    "            target = data.iloc[row, 4:].values.astype(int)\n",
    "            self.targets.append(target.tolist())\n",
    "\n",
    "\n",
    "    def get_datasets(self, rand=False):\n",
    "        final_dataset=[]        \n",
    "        for llm, opensmile, cps_f, cps_i, action, gamr, label in zip(self.dataset_llm, self.dataset_opensmile,self.dataset_cps_f, self.dataset_cps_i, self.dataset_action, self.dataset_gamr, self.targets):\n",
    "            final_dataset.append([llm, opensmile, cps_f, cps_i, action, gamr, label])\n",
    "            \n",
    "        if rand:\n",
    "            random.shuffle(final_dataset)\n",
    "        return final_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(dataset, root, file, llm=\"bert-base-uncased\"):\n",
    "\n",
    "    if LLM in file:\n",
    "        dataset.openLLM(root+\"/\"+file)\n",
    "    if 'oracle_opensmile' in file:\n",
    "        dataset.openSmile(root+\"/\"+file)\n",
    "    elif 'CPS_Oracle' in file:\n",
    "        dataset.openCPS(root+\"/\"+file)\n",
    "    elif \"Actions_Oracle_vectors\" in file:\n",
    "        dataset.openAction(root+\"/\"+file)\n",
    "    elif \"GAMR_Oracle_vectors\" in file:\n",
    "        dataset.openGAMR(root+\"/\"+file)\n",
    "    elif 'CG.csv' in file:\n",
    "        dataset.openTarget(root+\"/\"+file)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datasets = DATA()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for root, dirs,files in (os.walk(os. getcwd())):\n",
    "    if \"Group_\" in root:\n",
    "        if \"asr\" not in root:\n",
    "            print(root)\n",
    "            for file in files:\n",
    "                read_data(train_datasets, root, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_datasets.dataset_llm) == len(train_datasets.dataset_opensmile) == len(train_datasets.dataset_cps_f) == len(train_datasets.dataset_cps_i) == len(train_datasets.dataset_action) == len(train_datasets.dataset_gamr) == len(train_datasets.targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaler = MinMaxScaler()\n",
    "# train_datasets.dataset_llm = scaler.fit_transform(train_datasets.dataset_llm).tolist()\n",
    "# scaler = MinMaxScaler()\n",
    "# train_datasets.dataset_opensmile = scaler.fit_transform(train_datasets.dataset_opensmile).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = train_datasets.get_datasets()\n",
    "l = [a+b+c+d+e+f for a,b,c,d,e,f,g in l]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "958"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_folds = 3\n",
    "fold_len = int(np.ceil(len(train_datasets.get_datasets())/n_folds))\n",
    "fold_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = [train_datasets.get_datasets()[x:x+fold_len] for x in range(0, len(train_datasets.get_datasets()), fold_len)] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recurrent model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_list = train_datasets.get_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_list = [[a+b+c+d+e+f, g] for a,b,c,d,e,f,g in train_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "variable window size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_window_size(window_size):\n",
    "    rec_train_list = []\n",
    "    pad = [[0]*1199, [0]*7]\n",
    "\n",
    "    for utt_id in range(len(train_list)):\n",
    "        aux = []\n",
    "        for i in range(window_size):\n",
    "            if utt_id == i:\n",
    "                for _ in range(window_size-utt_id):\n",
    "                    aux.append(pad)\n",
    "        for i in range(window_size):\n",
    "            if len(aux) == i:\n",
    "                aux.append(train_list[utt_id - window_size + i])\n",
    "        aux.append(train_list[utt_id])\n",
    "        rec_train_list.append(aux)\n",
    "    \n",
    "    return rec_train_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_train_list = change_window_size(window_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1199"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_size+88+19+3+78+243"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "class rec_dataset(Dataset):\n",
    "    def __init__(self,xy=None):\n",
    "        self.utt_bert_l = []\n",
    "        self.utt_open_l = []\n",
    "        self.utt_cps_f_l = []\n",
    "        self.utt_cps_i_l = []\n",
    "        self.utt_action_l = []\n",
    "        self.utt_gamr_l = []\n",
    "        self.utt_y_l = []\n",
    "\n",
    "        for utt in xy:\n",
    "            bert_tmp = []\n",
    "            open_tmp = []\n",
    "            cps_i_tmp = []\n",
    "            cps_f_tmp = []\n",
    "            action_tmp = []\n",
    "            gamr_tmp = []\n",
    "            y_tmp = []\n",
    "            for utt_id in range(window_size+1):\n",
    "                bert_tmp.append(utt[utt_id][0][:llm_size])\n",
    "                open_tmp.append(utt[utt_id][0][llm_size:llm_size+89])\n",
    "                cps_i_tmp.append(utt[utt_id][0][llm_size+89:llm_size+89+19])\n",
    "                cps_f_tmp.append(utt[utt_id][0][llm_size+89+19:llm_size+89+20+3])\n",
    "                action_tmp.append(utt[utt_id][0][llm_size+89+20+3:llm_size+89+20+3+78])\n",
    "                gamr_tmp.append(utt[utt_id][0][llm_size+89+20+3+78:])\n",
    "                y_tmp.append(utt[utt_id][1])\n",
    "            self.utt_bert_l.append(bert_tmp)\n",
    "            self.utt_open_l.append(open_tmp)\n",
    "            self.utt_cps_f_l.append(cps_i_tmp)\n",
    "            self.utt_cps_i_l.append(cps_f_tmp)\n",
    "            self.utt_action_l.append(gamr_tmp)\n",
    "            self.utt_gamr_l.append(action_tmp)\n",
    "            self.utt_y_l.append(y_tmp)\n",
    "        self.utt_bert_l = torch.from_numpy(np.asarray(self.utt_bert_l))\n",
    "        self.utt_open_l = torch.from_numpy(np.asarray(self.utt_open_l))\n",
    "        self.utt_cps_f_l = torch.from_numpy(np.asarray(self.utt_cps_f_l))\n",
    "        self.utt_cps_i_l = torch.from_numpy(np.asarray(self.utt_cps_i_l))\n",
    "        self.utt_action_l = torch.from_numpy(np.asarray(self.utt_action_l))\n",
    "        self.utt_gamr_l = torch.from_numpy(np.asarray(self.utt_gamr_l))\n",
    "        self.utt_y_l = torch.from_numpy(np.asarray(self.utt_y_l))\n",
    "\n",
    "\n",
    "\n",
    "        self.len=len(self.utt_bert_l)\n",
    "        \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.utt_bert_l[index], self.utt_open_l[index], self.utt_cps_f_l[index], self.utt_cps_i_l[index], self.utt_action_l[index], self.utt_gamr_l[index], self.utt_y_l[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_train_loader = DataLoader(dataset=rec_dataset(rec_train_list),batch_size=16,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class rec_common_ground(nn.Module):\n",
    "    def __init__(self, lin_layers=True, bert_b=True, opensmile_b=True, cps_b=True, cps_f_b=True, action_b=True, gamr_b=True, output_size=1):\n",
    "        super(rec_common_ground, self).__init__()\n",
    "        self.lin_bert1 = nn.Linear(llm_size, 256)\n",
    "        self.lin_bert2 = nn.Linear(256, 256)\n",
    "        self.lstm_bert = nn.LSTM(input_size=256, batch_first=True, hidden_size=256)\n",
    "        if not lin_layers:\n",
    "            self.lstm_bert = nn.LSTM(input_size=llm_size, batch_first=True, hidden_size=256)\n",
    "\n",
    "\n",
    "        self.lin_open1 = nn.Linear(88, 256)\n",
    "        self.lin_open2 = nn.Linear(256, 256)\n",
    "        self.lstm_opensmile = nn.LSTM(input_size=256, batch_first=True, hidden_size=256)\n",
    "        if not lin_layers:\n",
    "            self.lstm_opensmile = nn.LSTM(input_size=88, batch_first=True, hidden_size=256)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        cps_size = 19\n",
    "        if cps_f_b:\n",
    "            cps_size = 3\n",
    "        self.lin_cps1 = nn.Linear(cps_size, 256)\n",
    "        self.lin_cps2 = nn.Linear(256, 256)\n",
    "        self.lstm_cps = nn.LSTM(input_size=256, batch_first=True, hidden_size=256)\n",
    "        if not lin_layers:\n",
    "            self.lstm_cps = nn.LSTM(input_size=cps_size, batch_first=True, hidden_size=256)\n",
    "\n",
    "\n",
    "        self.lin_action1 = nn.Linear(78, 256)\n",
    "        self.lin_action2 = nn.Linear(256, 256)\n",
    "        self.lstm_action = nn.LSTM(input_size=256, batch_first=True, hidden_size=256)\n",
    "        if not lin_layers:\n",
    "            self.lstm_action = nn.LSTM(input_size=78, batch_first=True, hidden_size=256)\n",
    "\n",
    "        self.lin_gamr1 = nn.Linear(243, 256)\n",
    "        self.lin_gamr2 = nn.Linear(256, 256)\n",
    "        self.lstm_gamr = nn.LSTM(input_size=256, batch_first=True, hidden_size=256)\n",
    "        if not lin_layers:\n",
    "            self.lstm_gamr = nn.LSTM(input_size=243, batch_first=True, hidden_size=256)\n",
    "\n",
    "\n",
    "\n",
    "        n_modals = int(bert_b) + int(opensmile_b) + int(cps_b) + int(action_b) + int(gamr_b)\n",
    "        self.ff1 = nn.Linear(256*n_modals, 512)\n",
    "        self.ff2 = nn.Linear(512, 512)\n",
    "        self.act1 = nn.Tanh()\n",
    "        self.act2 = nn.SiLU()\n",
    "        self.classifier = nn.Linear(512, output_size)\n",
    "\n",
    "\n",
    "    def forward(self, utt_bert_l, utt_open_l, utt_cps_l, utt_action_l, utt_gamr_l, lin_layers=True, bert_b=True, opensmile_b=True, cps_b=True, cps_f_b=True, action_b=True, gamr_b=True):\n",
    "\n",
    "        if bert_b:\n",
    "            if lin_layers:\n",
    "                utt_bert_l = [self.lin_bert1(utt_x_bert) for utt_x_bert in utt_bert_l]\n",
    "                utt_bert_l = [self.lin_bert2(utt_x_bert) for utt_x_bert in utt_bert_l]\n",
    "                utt_bert_l = [self.relu(utt_x_bert) for utt_x_bert in utt_bert_l]\n",
    "            bert = torch.stack(tuple(utt_bert_l), dim=1)\n",
    "            bert = self.lstm_bert(bert)[1][0][0]\n",
    "\n",
    "        if opensmile_b:\n",
    "            if lin_layers:\n",
    "                utt_open_l = [self.lin_open1(utt_x_open) for utt_x_open in utt_open_l]\n",
    "                utt_open_l = [self.lin_open2(utt_x_open) for utt_x_open in utt_open_l]\n",
    "                utt_open_l = [self.relu(utt_x_open) for utt_x_open in utt_open_l]\n",
    "            opensmile = torch.stack(tuple(utt_open_l), dim=1)\n",
    "            opensmile = self.lstm_opensmile(opensmile)[1][0][0]\n",
    "\n",
    "        if cps_b:\n",
    "            if lin_layers:\n",
    "                utt_cps_l = [self.lin_cps1(utt_x_cps) for utt_x_cps in utt_cps_l]\n",
    "                utt_cps_l = [self.lin_cps2(utt_x_cps) for utt_x_cps in utt_cps_l]\n",
    "                utt_cps_l = [self.relu(utt_x_cps) for utt_x_cps in utt_cps_l]\n",
    "            cps = torch.stack(tuple(utt_cps_l), dim=1)\n",
    "            cps = self.lstm_cps(cps)[1][0][0]\n",
    "\n",
    "        if action_b:\n",
    "            if lin_layers:\n",
    "                utt_action_l = [self.lin_action1(utt_x_action) for utt_x_action in utt_action_l]\n",
    "                utt_action_l = [self.lin_action2(utt_x_action) for utt_x_action in utt_action_l]\n",
    "                utt_action_l = [self.relu(utt_x_action) for utt_x_action in utt_cps_l]\n",
    "            action = torch.stack(tuple(utt_action_l), dim=1)\n",
    "            action = self.lstm_action(action)[1][0][0]\n",
    "\n",
    "        if gamr_b:\n",
    "            if lin_layers:\n",
    "                utt_gamr_l = [self.lin_gamr1(utt_x_gamr) for utt_x_gamr in utt_gamr_l]\n",
    "                utt_gamr_l = [self.lin_gamr2(utt_x_gamr) for utt_x_gamr in utt_gamr_l]\n",
    "                utt_gamr_l = [self.relu(utt_x_gamr) for utt_x_gamr in utt_gamr_l]\n",
    "            gamr = torch.stack(tuple(utt_gamr_l), dim=1)\n",
    "            gamr = self.lstm_gamr(gamr)[1][0][0]\n",
    "\n",
    "        modals = []\n",
    "        if bert_b: modals.append(bert)\n",
    "        if opensmile_b: modals.append(opensmile)\n",
    "        if cps_b: modals.append(cps)\n",
    "        if action_b: modals.append(action)\n",
    "        if gamr_b: modals.append(gamr)\n",
    "\n",
    "\n",
    "        x = torch.hstack(tuple(modals))\n",
    "        x = self.ff1(x)\n",
    "        x = self.act1(x)\n",
    "        x = self.ff2(x)\n",
    "        x = self.act2(x)\n",
    "        predict = self.classifier(x)\n",
    "\n",
    "        return predict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rec_train(model, total_epochs, lr, train_iterator, class_to_eval, lin_layers, bert_b, opensmile_b, cps_b, cps_f_b, action_b, gamr_b, output_size):\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    epoch_loss = []\n",
    "    nepochs = 0\n",
    "    model = model.to(device)\n",
    "    model.train()\n",
    "    criterion = nn.BCELoss(reduction='mean').to(device)\n",
    "    if output_size != 1:\n",
    "        criterion = nn.CrossEntropyLoss(reduction='mean').to(device)\n",
    "    while nepochs < total_epochs :\n",
    "        optimizer.zero_grad()\n",
    "        batch_loss = []\n",
    "        for batch_idx, (utt_bert_l, utt_open_l, utt_cps_f_l, utt_cps_i_l, utt_action_l, utt_gamr_l, utt_y_l) in enumerate(train_iterator):\n",
    "            utt_cps_l = utt_cps_f_l\n",
    "            if not cps_f_b:\n",
    "                utt_cps_l = utt_cps_i_l\n",
    "            output = model(utt_bert_l, utt_open_l, utt_cps_l, utt_action_l, utt_gamr_l, lin_layers, bert_b, opensmile_b, cps_b, cps_f_b, action_b, gamr_b)\n",
    "            target = utt_y_l[-1]\n",
    "            if output_size == 1:\n",
    "                target_binary = torch.zeros(target.size()[0], 1).to(device)\n",
    "                for i,t in enumerate(target):\n",
    "                    target_binary[i] = torch.Tensor([t[class_to_eval]])\n",
    "                loss = criterion(torch.sigmoid(output).to(device), target_binary)\n",
    "            else:\n",
    "                loss = criterion(output, target)\n",
    "            batch_loss.append(loss)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        nepochs += 1\n",
    "        epoch_loss.append(sum(batch_loss)/len(batch_loss))\n",
    "    return model, epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rec_test(model, test_iterator, class_to_eval, lin_layers, bert_b, opensmile_b, cps_b, cps_f_b, action_b, gamr_b, output_size):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        true, pred = None, None\n",
    "        for batch_idx, (utt_bert_l, utt_open_l, utt_cps_f_l, utt_cps_i_l, utt_action_l, utt_gamr_l, utt_y_l) in enumerate(test_iterator):\n",
    "            utt_cps_l = utt_cps_f_l\n",
    "            if not cps_f_b:\n",
    "                utt_cps_l = utt_cps_i_l\n",
    "            output = model(utt_bert_l, utt_open_l, utt_cps_l, utt_action_l, utt_gamr_l, lin_layers, bert_b, opensmile_b, cps_b, cps_f_b, action_b, gamr_b)\n",
    "            target = utt_y_l[-1]\n",
    "            if output_size == 1:\n",
    "                pred_tmp = torch.sigmoid(output)\n",
    "                true_binary = torch.zeros(target.size()[0], 1)\n",
    "                for i, t in enumerate(target):\n",
    "                    true_binary[i] = torch.Tensor([t[class_to_eval]])\n",
    "                if true == None:\n",
    "                    true = true_binary\n",
    "                    pred = pred_tmp > 0.5\n",
    "                    pred_probs = pred_tmp\n",
    "\n",
    "                else :\n",
    "                    true = torch.cat((true, true_binary))\n",
    "                    pred = torch.cat((pred, pred_tmp > 0.5))\n",
    "                    pred_probs = torch.cat((pred_probs, pred_tmp))\n",
    "                    \n",
    "            \n",
    "            else:\n",
    "                softmax = torch.nn.Softmax(dim=1)\n",
    "                target_ = torch.clone(target)\n",
    "                preds_tmp = softmax(output).cpu().detach().numpy()\n",
    "                true_tmp = target_.cpu().numpy()\n",
    "\n",
    "                if true == None:\n",
    "                    true = torch.argmax(torch.from_numpy(true_tmp), dim=1)\n",
    "                    pred = torch.argmax(torch.from_numpy(preds_tmp), dim=1)\n",
    "                    preds_probs = torch.tensor(preds_tmp)\n",
    "                else:\n",
    "                    true = torch.cat((true, torch.argmax(torch.from_numpy(true_tmp), dim=1)))\n",
    "                    pred = torch.cat((pred, torch.argmax(torch.from_numpy(preds_tmp), dim=1)))\n",
    "                    preds_probs = torch.cat((preds_probs, torch.tensor(preds_tmp)))\n",
    "        \n",
    "    return true, pred, pred_probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LLMs = [\"bert-base-uncased\", \"bert-large-uncased\", \"albert\", \"roberta\", \"xlm\"]\n",
    "n_folds = [5, 6, 7] # train test valid\n",
    "window_sizes = [1, 3, 5, 7, 9]\n",
    "lrs = [0.01, 0.001, 0.0001]\n",
    "output_sizes = [1, 8, 9] # [binary, multinomial without negatives, multinomial with all utterances]\n",
    "bert_b = [True, False]\n",
    "opensmile_b = [True, False]\n",
    "action_b = [True, False]\n",
    "gamr_b = [True, False]\n",
    "cps_b = [True, False]\n",
    "cps_f_b = [True, False]\n",
    "lin_layers = [True, False]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer, llm, llm_size = change_llm(LLM)\n",
    "fold_len = int(np.ceil(len(train_datasets.get_datasets())/n_folds))\n",
    "folds = [train_datasets.get_datasets()[x:x+fold_len] for x in range(0, len(train_datasets.get_datasets()), fold_len)]\n",
    "\n",
    "for class_to_eval in range(6):\n",
    "    print(f\"Results for class {class_to_eval}\")\n",
    "    for k in range(len(folds)):\n",
    "        train_l = []\n",
    "        for i in range(len(folds)):\n",
    "            if i != k:\n",
    "                train_l += folds[i]\n",
    "        test_l = folds[k]\n",
    "        train_loader = DataLoader(dataset=rec_dataset(train_l),batch_size=16,shuffle=False)\n",
    "        test_loader = DataLoader(dataset=rec_dataset(test_l),batch_size=16,shuffle=False)\n",
    "        \n",
    "        \n",
    "        \n",
    "        model = rec_common_ground(lin_layers, bert_b, opensmile_b, output_size).to(device)\n",
    "        rec_train(model, 60, train_loader, class_to_eval)\n",
    "        true, pred, pred_probs = rec_test(model, test_loader, class_to_eval)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # try:\n",
    "        #     auroc = roc_auc_score(true, torch.nan_to_num(pred_probs, 0.5))\n",
    "        # except:\n",
    "        #     auroc =  0.5\n",
    "        # print(auroc)\n",
    "        accuracy = accuracy_score(true.to(\"cpu\"), pred.to(\"cpu\"))\n",
    "        print(accuracy)\n",
    "\n",
    "\n",
    "\n",
    "        # torch.save(model, f\"save/binary_all_features/{Class_to_eval}_{filt}_{NN_size}_{epochs}_{group_holdout}.pt\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
